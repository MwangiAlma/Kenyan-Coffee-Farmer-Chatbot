{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MwangiAlma/Kenyan-Coffee-Farmer-Chatbot/blob/main/Notebooks/CoffeeTextprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Coffee Chatbot Text Pre-Processing\n",
        "\n",
        " The objective of this notebook is to build a functional multilingual chatbot using the preprocessed English and Swahili question data and their TF-IDF representations. This will involve:\n",
        "\n",
        "1. User Input Preprocessing: Developing a function to clean and prepare new user queries, consistent with how the dataset was processed.\n",
        "2. Language Detection: Implementing a method to automatically determine the language of an incoming user query (English or Swahili).\n",
        "Similarity Search: Utilizing TF-IDF vectors and cosine similarity to identify the most relevant question within our dataset that matches the preprocessed user query.\n",
        "3. Response Retrieval: Extracting and returning the appropriate English or Swahili response corresponding to the most similar question found."
      ],
      "metadata": {
        "id": "YgJEzDwFkDl3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1G3__VdbfZcs",
        "outputId": "c3e80d0d-a936-4fbb-eabb-77e9ee49bcfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/981.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=bc0bf3afd36c370e583e1c007356a10f0c99d15420ba90f9bd93f206a40350ae\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: python-docx, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, langdetect, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed langdetect-1.0.9 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 python-docx-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install python-docx transformers sentence-transformers scikit-learn pandas nltk langdetect\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "import docx as Document\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from langdetect import detect, DetectorFactory\n",
        "import requests\n",
        "\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "# NLTK resources download. These ensure the needed linquistic resources are available.\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlD2rOE0gLNB",
        "outputId": "94d9499b-23ab-404b-c401-57b72befbc65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 - Data loading and standardization\n",
        "try:\n",
        "    df = pd.read_csv('/content/drive/MyDrive/Chatbot_dataset.csv', encoding='latin1') # Loads data to the dataset from CSV file.\n",
        "    print(\"Dataset loaded successfully from Chatbot_dataset.csv\")\n",
        "    print(\"Original Columns in the DataFrame:\")\n",
        "    print(df.columns.tolist())\n",
        "\n",
        "    # Renames the columns from the format in the CSV file to an easier format the file can understand.\n",
        "    column_rename_map = {\n",
        "        'Question (EN)': 'Question_EN',\n",
        "        'Question (SW)': 'Question_SW',\n",
        "        'Response (EN)': 'Response_EN',\n",
        "        'Response (SW)': 'Response_SW'\n",
        "    }\n",
        "    df = df.rename(columns=column_rename_map)\n",
        "\n",
        "    unnamed_cols = [col for col in df.columns if 'Unnamed:' in col]\n",
        "\n",
        "    if unnamed_cols:\n",
        "        df = df.drop(columns=unnamed_cols)\n",
        "        print(f\"\\nDropped unnecessary columns: {unnamed_cols}\")\n",
        "\n",
        "    if 'Variations_EN' not in df.columns:\n",
        "        df['Variations_EN'] = [[] for _ in range(len(df))]\n",
        "        print(\"\\nAdded 'Variations_EN' column (initialized as empty lists).\")\n",
        "\n",
        "    print(\"\\nStandardized Columns in DataFrame after renaming and dropping:\")\n",
        "    print(df.columns.tolist())\n",
        "    print(\"\\nFirst 5 rows of the DataFrame (with standardized columns):\")\n",
        "    print(df.head())\n",
        "\n",
        "    # defines list of column names\n",
        "    required_cols = ['Question_EN', 'Question_SW', 'Response_EN', 'Response_SW', 'Variations_EN']\n",
        "    for col in required_cols:\n",
        "        if col not in df.columns:\n",
        "            print(f\"WARNING (After Standardization): Required column '{col}' is still missing!\")\n",
        "        elif df[col].isnull().all() or (df[col].astype(str).str.strip() == '').all():\n",
        "            print(f\"WARNING (After Standardization): Column '{col}' appears to be entirely empty or contains only whitespace/NaNs!\")\n",
        "        elif df[col].isnull().any() or (df[col].astype(str).str.strip() == '').any():\n",
        "            print(f\"Note (After Standardization): Column '{col}' contains some empty/NaN values.\")\n",
        "\n",
        "# Error handling\n",
        "except FileNotFoundError: # the csv file doesn't exist\n",
        "    print(\"Error: Chatbot_dataset.csv not found. Please upload it or ensure the path is correct.\")\n",
        "    df = pd.DataFrame(columns=['Question_EN', 'Question_SW', 'Response_EN', 'Response_SW', 'Variations_EN'])\n",
        "except UnicodeDecodeError: # handles issues with decoding the csv file\n",
        "    print(\"Error: Could not decode the CSV file. It might be saved with a different encoding than 'latin1'.\")\n",
        "    print(\"Try changing 'encoding='latin1'' to 'encoding='cp1252'' or 'encoding='iso-8859-1''.\")\n",
        "    df = pd.DataFrame(columns=['Question_EN', 'Question_SW', 'Response_EN', 'Response_SW', 'Variations_EN'])\n",
        "except Exception as e: # handles any unexpected errors\n",
        "    print(f\"An unexpected error occurred during CSV loading or column standardization: {e}\")\n",
        "    df = pd.DataFrame(columns=['Question_EN', 'Question_SW', 'Response_EN', 'Response_SW', 'Variations_EN'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upgaX5VEggca",
        "outputId": "10cde160-6862-4fff-a93a-4291786a2a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded successfully from Chatbot_dataset.csv\n",
            "Original Columns in the DataFrame:\n",
            "['Topic', 'Question_EN', 'Question_SW', 'Response_EN', 'Response_SW']\n",
            "\n",
            "Added 'Variations_EN' column (initialized as empty lists).\n",
            "\n",
            "Standardized Columns in DataFrame after renaming and dropping:\n",
            "['Topic', 'Question_EN', 'Question_SW', 'Response_EN', 'Response_SW', 'Variations_EN']\n",
            "\n",
            "First 5 rows of the DataFrame (with standardized columns):\n",
            "                        Topic  \\\n",
            "0  Planting Coffee Seedlings    \n",
            "1  Planting Coffee Seedlings    \n",
            "2  Planting Coffee Seedlings    \n",
            "3  Planting Coffee Seedlings    \n",
            "4  Planting Coffee Seedlings    \n",
            "\n",
            "                                         Question_EN  \\\n",
            "0             When is the best time to plant coffee?   \n",
            "1  What is the right season to plant coffee seedl...   \n",
            "2                     When should I plant my coffee?   \n",
            "3         Which months are best for planting coffee?   \n",
            "4          Can I plant coffee during the dry season?   \n",
            "\n",
            "                                     Question_SW  \\\n",
            "0  Ni wakati gani mzuri zaidi wa kupanda kahawa?   \n",
            "1     Ni msimu gani wa kupanda mchele wa kahawa?   \n",
            "2           Ninapaswa kupanda kahawa yangu lini?   \n",
            "3    Miezi gani ni bora zaidi ya kupanda kahawa?   \n",
            "4      Naweza kupanda kahawa wakati wa kiangazi?   \n",
            "\n",
            "                                         Response_EN  \\\n",
            "0  The best time to plant coffee seedlings is dur...   \n",
            "1  The best time to plant coffee seedlings is dur...   \n",
            "2  The best time to plant coffee seedlings is dur...   \n",
            "3  The best months to plant coffee are during the...   \n",
            "4  It is not recommended to plant coffee during t...   \n",
            "\n",
            "                                         Response_SW Variations_EN  \n",
            "0  Wakati bora wa kupanda michele ya kahawa ni wa...            []  \n",
            "1  Wakati bora wa kupanda michele ya kahawa ni wa...            []  \n",
            "2  Wakati bora wa kupanda michele ya kahawa ni wa...            []  \n",
            "3  Miezi bora ya kupanda kahawa ni wakati wa msim...            []  \n",
            "4  Haipendekezwi kupanda kahawa wakati wa kiangaz...            []  \n",
            "Note (After Standardization): Column 'Question_EN' contains some empty/NaN values.\n",
            "Note (After Standardization): Column 'Question_SW' contains some empty/NaN values.\n",
            "Note (After Standardization): Column 'Response_EN' contains some empty/NaN values.\n",
            "Note (After Standardization): Column 'Response_SW' contains some empty/NaN values.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3 : Removing stopwords and Lemmatization\n",
        "\n",
        "stop_words_en = set(stopwords.words('english')) # Gets a set of stopwords from NLTK corpus\n",
        "lemmatizer_en = WordNetLemmatizer() # Reduces the words to their short form\n",
        "\n",
        "try:\n",
        "    swahili_stopwords_url = \"https://raw.githubusercontent.com/dohliam/more-stoplists/master/sw/sw.txt\" # Fetching swahili stopwords from this github link\n",
        "    response = requests.get(swahili_stopwords_url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    stop_words_sw = set(response.text.strip().split('\\n'))\n",
        "    print(\"Swahili stopwords loaded successfully from GitHub (new URL).\")\n",
        "\n",
        "# Error handling\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching Swahili stopwords from URL: {e}\")\n",
        "    print(\"Proceeding with an empty set for Swahili stopwords. Swahili preprocessing might be less effective.\")\n",
        "    stop_words_sw = set()\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred while loading Swahili stopwords: {e}\")\n",
        "    print(\"Proceeding with an empty set for Swahili stopwords. Swahili preprocessing might be less effective.\")\n",
        "    stop_words_sw = set()\n",
        "\n",
        "print(\"NLTK Stopwords (English) and WordNetLemmatizer (English) initialized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMWsXS0ogobR",
        "outputId": "ba9dafd5-11e1-4ee8-8251-1f01cdc4bee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Swahili stopwords loaded successfully from GitHub (new URL).\n",
            "NLTK Stopwords (English) and WordNetLemmatizer (English) initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4 : Text preprocessing\n",
        "\n",
        "def preprocess_english_text(text):\n",
        "  if pd.isna(text): # Checks if the input is Not a Number\n",
        "    return \"\"\n",
        "  text = str(text).lower()\n",
        "  words = nltk.word_tokenize(text)\n",
        "  words = nltk.word_tokenize(text)\n",
        "  words = [lemmatizer_en.lemmatize(word) for word in words if word.isalpha() and word not in stop_words_en]\n",
        "  return \"\".join(words)\n",
        "\n",
        "def preprocess_swahili_text(text):\n",
        "  if pd.isna(text):\n",
        "    return \"\"\n",
        "  text = re.sub(r'[^a-z\\s]', '', text)\n",
        "  words = text.split()\n",
        "\n",
        "  # Filters out Swahili stop_words\n",
        "  words = [word for word in words if word not in stop_words_sw]\n",
        "  return \" \".join(words)\n",
        "\n",
        "# Checks if 'Question_EN' column exixts in the DataFrame\n",
        "if 'Question_EN' in df.columns:\n",
        "  df['Processed_Question_EN'] = df['Question_EN'].apply(preprocess_english_text)\n",
        "  print(\"English text preprocessing applied to 'Question_EN' column\")\n",
        "  print(\"\\nFirst 5 rows with 'Questions_EN' and 'Processed_Question_EN':\")\n",
        "  print(df[['Question_EN', 'Processed_Question_EN']].head()) # Prints the first five english questions and the lemmatized form\n",
        "else:\n",
        "  print(\"Error: 'Question_EN' column not found. Cannot apply English preprocessing\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Checks if 'Question_SW' column exists in the DataFrame\n",
        "if 'Question_SW' in df.columns:\n",
        "  df['Processed_Question_SW'] = df['Question_SW'].apply(preprocess_swahili_text)\n",
        "  print(\"Swahili text preprocessing applied to 'Question_SW' column.\")\n",
        "  print(\"\\nFirst 5 rows with 'Questions_SW' and 'Processed_Question_SW':\")\n",
        "  print(df[['Question_SW', 'Processed_Question_SW']].head()) # Prints the first five swahili questions and the lemmatized form\n",
        "else:\n",
        "  print(\"Error: 'Question_SW' column not found. Cannot apply Swahili preprocessing.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc4sQjbVgthk",
        "outputId": "e5e935d3-e8fa-4fea-c804-1f7bdbe6f0db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English text preprocessing applied to 'Question_EN' column\n",
            "\n",
            "First 5 rows with 'Questions_EN' and 'Processed_Question_EN':\n",
            "                                         Question_EN  \\\n",
            "0             When is the best time to plant coffee?   \n",
            "1  What is the right season to plant coffee seedl...   \n",
            "2                     When should I plant my coffee?   \n",
            "3         Which months are best for planting coffee?   \n",
            "4          Can I plant coffee during the dry season?   \n",
            "\n",
            "            Processed_Question_EN  \n",
            "0             besttimeplantcoffee  \n",
            "1  rightseasonplantcoffeeseedling  \n",
            "2                     plantcoffee  \n",
            "3         monthbestplantingcoffee  \n",
            "4            plantcoffeedryseason  \n",
            "--------------------------------------------------\n",
            "Swahili text preprocessing applied to 'Question_SW' column.\n",
            "\n",
            "First 5 rows with 'Questions_SW' and 'Processed_Question_SW':\n",
            "                                     Question_SW  \\\n",
            "0  Ni wakati gani mzuri zaidi wa kupanda kahawa?   \n",
            "1     Ni msimu gani wa kupanda mchele wa kahawa?   \n",
            "2           Ninapaswa kupanda kahawa yangu lini?   \n",
            "3    Miezi gani ni bora zaidi ya kupanda kahawa?   \n",
            "4      Naweza kupanda kahawa wakati wa kiangazi?   \n",
            "\n",
            "                Processed_Question_SW  \n",
            "0         i gani mzuri kupanda kahawa  \n",
            "1  i msimu gani kupanda mchele kahawa  \n",
            "2        inapaswa kupanda kahawa lini  \n",
            "3       iezi gani bora kupanda kahawa  \n",
            "4       aweza kupanda kahawa kiangazi  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_english_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses English text: converts to lowercase and removes punctuation.\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "def preprocess_swahili_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses Swahili text: converts to lowercase and removes punctuation.\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# This ensures the 'Processed_Question_EN' and 'Processed_Question_SW' columns exist.\n",
        "df['Processed_Question_EN'] = df['Question_EN'].apply(preprocess_english_text)\n",
        "\n",
        "if 'Question_SW' in df.columns:\n",
        "    df['Processed_Question_SW'] = df['Question_SW'].apply(preprocess_swahili_text)\n",
        "else:\n",
        "    print(\"Warning: 'Question_SW' column not found, skipping Swahili preprocessing.\")\n",
        "    df['Processed_Question_SW'] = \"\" # Create an empty column to prevent errors later\n",
        "\n",
        "if 'Variations_EN' in df.columns and not df['Variations_EN'].empty:\n",
        "    df['Processed_Variations_EN'] = df['Variations_EN'].apply(\n",
        "        lambda variations: [preprocess_english_text(v) for v in variations if pd.notna(v)] if isinstance(variations, list) else []\n",
        "    )\n",
        "else:\n",
        "    print(\"Warning: 'Variations_EN' column not found or is empty. Skipping preprocessing for variations.\")\n",
        "\n",
        "# Initializes TF-IDF vectorizer for English, max_features considers the top 5000 most frequent terms\n",
        "tfidf_vectorizer_en = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "corpus_en = df['Processed_Question_EN'].tolist()\n",
        "\n",
        "if 'Processed_Variations_EN' in df.columns:\n",
        "    for variations_list in df['Processed_Variations_EN']:\n",
        "        if variations_list is not None:\n",
        "            for variation in variations_list:\n",
        "                if pd.notna(variation) and variation.strip():\n",
        "                    corpus_en.append(variation)\n",
        "\n",
        "# Fits a TF-IDF vectorizer to the English corpus (questions + variations) then transforms the questions to a TF-IDF matrix.\n",
        "# Learns the vocabulary and IDF values and then transforms them to numerical vectors\n",
        "tfidf_matrix_en_full = tfidf_vectorizer_en.fit_transform(corpus_en)\n",
        "tfidf_matrix_questions_en = tfidf_vectorizer_en.transform(df['Processed_Question_EN'])\n",
        "\n",
        "print(\"English TF-IDF Vectorization complete.\")\n",
        "print(f\"English TF-IDF matrix shape (for all processed text, including variations): {tfidf_matrix_en_full.shape}\")\n",
        "print(f\"English TF-IDF matrix shape (for questions): {tfidf_matrix_questions_en.shape}\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Initializes TF-IDF vectorizer for Swahili, max_features considers the top 5000 most frequent terms\n",
        "tfidf_vectorizer_sw = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "if 'Processed_Question_SW' in df.columns:\n",
        "    corpus_sw = df['Processed_Question_SW'].tolist()\n",
        "    tfidf_matrix_sw_full = tfidf_vectorizer_sw.fit_transform(corpus_sw)\n",
        "    tfidf_matrix_questions_sw = tfidf_vectorizer_sw.transform(df['Processed_Question_SW'])\n",
        "\n",
        "    print(\"Swahili TF-IDF Vectorization complete.\")\n",
        "    print(f\"Swahili TF-IDF matrix shape (for all processed text): {tfidf_matrix_sw_full.shape}\")\n",
        "    print(f\"Swahili TF-IDF matrix shape (for processed questions only): {tfidf_matrix_questions_sw.shape}\")\n",
        "\n",
        "else:\n",
        "    print(\"Warning: 'Processed_Question_SW' column not found. Swahili TF-IDF not created\")\n",
        "    tfidf_vectorizer_sw = None\n",
        "    tfidf_matrix_questions_sw = None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RB-rabuOg1S3",
        "outputId": "f0c2724b-e32b-45f0-893c-187a18327a42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English TF-IDF Vectorization complete.\n",
            "English TF-IDF matrix shape (for all processed text, including variations): (448, 607)\n",
            "English TF-IDF matrix shape (for questions): (448, 607)\n",
            "--------------------------------------------------\n",
            "Swahili TF-IDF Vectorization complete.\n",
            "Swahili TF-IDF matrix shape (for all processed text): (448, 846)\n",
            "Swahili TF-IDF matrix shape (for processed questions only): (448, 846)\n"
          ]
        }
      ]
    }
  ]
}